%!TEX root=nips_2016.tex
\section{A recurrent neural network for discrete sequences}
In this section we describe how to construct a generative adversarial network (GAN) that is able to generate text from random noise samples. We also give a simple algorithm to train our model, inspired by recent work in adversarial modeling.

\begin{figure*}[t!]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{lstm_gan.pdf}}
%\vspace{-2ex}
\caption{(\emph{Top}): The classic LSTM model during the prediction phase. Each LSTM unit (shown as a blue box) makes a prediction based on the input it as seen in the past. This prediction is then used as input to the next unit, which makes its own prediction, and so on. (\emph{Bottom}): Our generative model for discrete sequences. At the beginning we draw a pair of samples which are fed into the network in place of the initial cell state $C_0$ and hidden state $h_0$. Our trained network takes these samples and uses them to generate an initial character, this generated character is fed to the next cell in the LSTM as input, and so on.}
%\vspace{-5ex}
\label{figure.prediction}
\end{center}
\end{figure*}



Our generative model is based on a Long Short Term Memory (LSTM) recurrent neural network \cite{hochreiter1997long}, shown in the top of Figure~\ref{figure.prediction}. Specifically, an LSTM can be constructed to sequentially predict the next element in a discrete sequence, given the previous elements. Instead, our generative model takes as input a sample-pair which effectively replace the initial cell and hidden states. From this sample our generator constructs a sequence by successively feeding its predictions as input to the following LSTM unit. The immediate difficulty is how to train this generative network to sample realistic sequences.

\subsection*{Generative adversarial modeling}

Given a set of $n$ data points $\{ \x_1, \x_2 \ldots, \x_n\}$ independently and identically drawn from a $d$-dimensional distribution $p(\x)$ over the reals, the goal of generative modeling is to learn a distribution $q(\x)$ that accuratley approximates $p(\x)$. The framework of generative adversarial modeling has been shown to yield models $q(\x)$ that generate amazingly realistic data points. The adversarial training idea is straight-forward. First, we are going to learn a so-called \emph{generator} $G$ that transforms samples from a simple, known distribution (e.g., a uniform or Gaussian distribution) into samples that approximate those drawn from $p(\x)$. Specifically, we define $q(\x) := G(\z)$, where $\z \sim U(0,1)^d$ (let $U(0,1)^d$ be the $d$-dimensional uniform distribution on the interval $[0,1]$). Second, to learn $G$ we will introduce a classifier we call the \emph{discriminator} $D$. The discriminator takes as input any real $d$-dimensional vector (this could be a generated input $G(\z)$ or a real one $\x$) and predicts the probability that the input is actually drawn from the real distribution $p(\x)$. It will be trained to take samples $G(\z)$ and real inputs $\x$ and accurately distinguish them. At the same time, the generator $G$ is trained so that it can fool the discriminator $D$ into thinking that a fake point it generated is real with high probability. Initially, the discriminator will be able to easily tell the fake points from the real ones and the generator is poor. However, as training progresses the generator uses this signal from the discriminator to determine how to generate more realistic samples. Eventually, the generator will generate samples so real that the discriminator will have a random chance of guessing if a generated point is real.

In the majority of the prior work, $G$ and $D$ are parameterized to be neural networks. Here $G$ and $D$ are both LSTMs with weight matrices $\Theta$ and $\Phi$, respectfully. We describe our adversarial training procedure in Algorithm~\ref{alg}, inspired by recent work on GANs \cite{sonderby2016amortised}. This algorithm can be shown in expectation to minimize the KL-divergence between $q(\z) \!=\! G(\z)$ and $p(\x)$.


%\begin{wrapfigure}{R}{0.45\textwidth}
%\vspace{-5ex}
% \begin{minipage}{0.45\textwidth}
\begin{algorithm}[H]                      % enter the algorithm environment
\caption{Generative Adversarial Network \cite{sonderby2016amortised}}          % give the algorithm a caption
\label{alg}                           % and a label for \ref{} commands later in the document
\begin{algorithmic}[1]                    % enter the algorithmic environment
%	\STATE \textbf{Input:} $\Vc$; $\Lambda \subseteq \mathbb{R}^d$; $T$; $(\epsilon,\delta)$; $\sigma^2_{\Vc,0}$; $\gamma_T$
%	\STATE $\mu_{\Vc,0} = 0$
	\STATE \textbf{data:} $\{ \x_1, \ldots, \x_n\} \sim p(\x)$,
	\STATE Generative LSTM network $G_\Theta$
	\STATE Discriminative LSTM network $D_\Phi$
	\WHILE{ loop until convergence }
		\STATE Sample mini-batch of inputs $B = \{\x_{B_1}, \ldots, \x_{B_m} \}$
		\STATE Sample noise $N = \{\z_{N_1}, \ldots, \z_{N_m}\}$
		\STATE Update discriminator $\Phi = \argmin_\Phi -\frac{1}{m} \sum_{\x \in B} \log D_\Phi(\x) - \frac{1}{m} \sum_{\z \in N} \log(1 - D_\Phi(G_\Theta(\z)))$
		\STATE Update generator $\Theta = \argmin_\Theta - \frac{1}{m} \sum_{\z \in N} \log \frac{D_\Phi(G_\Theta(\z))}{1-D_\Phi(G_\Theta(\z))}$
	\ENDWHILE
%	\STATE \textbf{Return:} $\tilde{\lambda},\tilde{v}$
\end{algorithmic}
\end{algorithm}
% \end{minipage}
%\vspace{-4ex}
%\end{wrapfigure}

Figure~\ref{figure.adversarial} shows a schematic of the adversarial training procedure for discrete sequences.


\begin{figure*}[t!]
\begin{center}
\centerline{\includegraphics[width=\textwidth]{GAN.pdf}}
%\vspace{-2ex}
\caption{The adversarial training procedure. Our generative model first generates a full-length sequence. This sequence is fed to the discriminator (also a LSTM), which predicts the probability of it being a real sequence. Additionally (not shown), the discriminator is fed real discrete sequence data, which again it predicts the probability of it being real. The weights the networks are modified to make the discriminator better at recognizing real from fake data, and to make the generator better at fooling the discriminator.}
%\vspace{-5ex}
\label{figure.adversarial}
\end{center}
\end{figure*}
